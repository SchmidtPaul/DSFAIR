---
title: "Repeated Measures RCBD"
output: 
  html_document:
    includes:
      in_header: header.html
      after_body: footer.html		
---

```{r, echo=FALSE, warning=FALSE, message=FALSE, purl=FALSE}
# formatting tables for html output
knitr::opts_chunk$set(message = FALSE, warning = FALSE, purl = TRUE)
options(knitr.kable.NA = '')
pacman::p_load(kableExtra, formattable ,htmltools)
var_colors <- c("#8cb369", "#f4a259", "#5b8e7d", "#bc4b51")
```

```{r}
# packages
pacman::p_load(conflicted, # handle conflicting functions
               agriTutorial, tidyverse,  # data import and handling
               nlme, glmmTMB, # linear mixed modelling
               mixedup, AICcmodavg, car, # linear mixed model processing
               emmeans, multcomp, # mean comparisons
               ggplot2, gganimate, gifski)  # (animated) plots 

conflict_prefer("select", "dplyr") # set select() from dplyr as default
conflict_prefer("filter", "dplyr") # set filter() from dplyr as default
```

# Data

The example in this chapter is taken from [*Example 4* in Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} (see also the [Agritutorial vigniette](https://cran.r-project.org/web/packages/agriTutorial/vignettes/agriTutorialVignette.pdf#%5B%7B%22num%22%3A81%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C28.346%2C813.543%2Cnull%5D){target="_blank"}). It considers data from a sorghum trial laid out as a randomized complete `block` design (5 blocks) with `variety` (4 sorghum varities) being the only treatment factor. Thus, we have a total of 20 `plot`s. It is important to note that our response variable (`y`), **the leaf area index, was assessed in five consecutive weeks on each plot** starting 2 weeks after emergence. Therefore, the dataset contains a total of 100 values and what we have here is longitudinal data, *a.k.a.* repeated measurements over time, *a.k.a.* a time series analysis.

As [Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} put it: *"the week factor is not a treatment factor that can be randomized. Instead, repeated measurements are taken on each plot on five consecutive occasions. Successive measurements on the same plot are likely to be serially correlated, and this means that for a reliable and efficient analysis of repeated-measures data we need to take proper account of the serial correlations between the repeated measures ([Piepho, Büchse & Richter, 2004](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1439-037X.2004.00097.x){target="_blank"}; [Pinheiro & Bates, 2000](http://library.mpib-berlin.mpg.de/toc/z2008_18.pdf){target="_blank"})."*

Please note that this example is also considered on the [MMFAIR website](https://schmidtpaul.github.io/MMFAIR/autoregressive_time_series.html){target="_blank"} but with more focus on how to use the different mixed model packages to fit the models.

## Import & Formatting

Note that I here decided to give more intuitive names and labels, but this is optional. We also create a column `unit` with one factor level per observation, which will be needed later when using `glmmTMB()`.

```{r}
# data (import via URL)
dat <- agriTutorial::sorghum %>% # data from agriTutorial package
  rename(block = Replicate, 
         weekF = factweek,  # week as factor
         weekN = varweek,   # week as numeric/integer
         plot  = factplot) %>% 
  mutate(variety = paste0("var", variety),    # variety id
         block   = paste0("block", block),    # block id
         weekF   = paste0("week", weekF),     # week id
         plot    = paste0("plot", plot),      # plot id
         unit    = paste0("obs", 1:n() )) %>% # obsevation id
  mutate_at(vars(variety:plot, unit), as.factor) %>% 
  as_tibble()

dat
```

## Exploring

In order to obtain a field layout of the trial, we would like use the `desplot()` function. Notice that for this we need two data columns that identify the `row` and `col` of each plot in the trial. These are unfortunately not given here, so that we cannot create the actual field layout plot. 

### descriptive tables

We could also have a look at the arithmetic means and standard deviations for yield per `variety`.

```{r}
dat %>% 
  group_by(variety) %>% 
  summarize(mean    = mean(y, na.rm=TRUE),
            std.dev = sd(y, na.rm=TRUE)) %>% 
  arrange(desc(mean)) %>% # sort
  print(n=Inf) # print full table
```

Furthermore, we could look at the arithmetic means for each week as follows:

```{r}
dat %>% 
  group_by(weekF, variety) %>% 
  summarize(mean = mean(y, na.rm=TRUE)) %>% 
  pivot_wider(names_from = weekF, values_from = mean)  
```

### descriptive plot

We can also create a plot to get a better feeling for the data. Note that here we could even decide to extend the ggplot to become an animated gif as follows:

```{r, class.source = "fold-hide", purl = FALSE, message = FALSE, warning = FALSE}
var_colors <- c("#8cb369", "#f4a259", "#5b8e7d", "#bc4b51")
names(var_colors) <- dat$variety %>% levels()

gganimate_plot <- ggplot(
  data = dat, aes(y = y, x = weekF,
                  group = variety,
                  color = variety)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(alpha = 0.5, size = 3) +
  scale_y_continuous(
    name = "Leaf area index",
    limits = c(0, 6.5),
    expand = c(0, 0),
    breaks = c(0:6)
  ) +
  scale_color_manual(values = var_colors) +
  theme_bw() +
  theme(legend.position = "bottom", 
        axis.title.x = element_blank()) +
  transition_time(weekN) +
  shadow_mark(exclude_layer = 2) 

animate(gganimate_plot, renderer = gifski_renderer()) # render gif
```

# Model building

Our goal is therefore to build a suitable model taking serial correlation into account. In order to do this, we will initially consider the model for a single time point. Then, we extend this model to account for multiple weeks by allowing for week-speficic effects. Finally, we further allow for serially correlated error terms. 

## Single week

When looking at data from a single time point (*e.g.* the first week), we merely have 20 observations from a randomized complete block design with a single treatment factor. It can therefore be analyzed with a simple one-way ANOVA (fixed `variety` effect) for randomized complete block designs (fixed `block` effect):

```{r}
dat.wk1 <- dat %>% filter(weekF == "week1") # subset data from first week only

mod.wk1 <- lm(formula = y ~ variety + block,
              data = dat.wk1)
```

We could now go on and look at the ANOVA via `anova(mod.wk1)` and it would indeed not be _wrong_ to simply repeat this for each week. Yet, one may not be satisfied with obtaining multiple ANOVA results - namely one per week. This is especially likeliy in case the results contradict each other, because *e.g.* the variety effects are found to be significant in only two out of five weeks. Therefore, one may want to analyze the entire dataset *i.e.* the multiple weeks jointly.

## Multiple weeks (MW)

Going from the single-week-analysis to jointly analyzing the entire dataset is more than just changing the `data =` statement in the model. This is because *"it is realistic to assume that the treatment effects evolve over time and thus are week-specific. Importantly, we must also allow for the block effects to change over time in an individual manner. For example, there could be fertility or soil type differences between blocks and these could have a smooth progressive or cumulative time-based effect on differences between the blocks dependent on factors such as temperature or rainfall"* [(Piepho & Edmondson, 2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"}. We implement this by taking the model in `mod.wk1` and multipyling each effect with `week`. Note that this is also true for the general interecept (µ) in `mod.wk1`, meaning that we would like to include one intercept per week, which can be achieved by simply adding `week` as a main effect as well. This leaves us with fixed main effects for `week`, `variety`, and `block`, as well as the week-specific effects of the latter two `week:variety` and `week:block`.

Finally, note that we are not doing anything about the model's error term at this point. More specifically this means that its variance structure is still the default **iid** (independent and identically distributed) - see [the summary on correlation/variance strucutres at the MMFAIR website](variance_structures.html){target="_blank"}.

> I decide to use the `glmmTMB` package to fit the linear models here, because I feel that their syntax is more intuitive. Note that one may also use the `nlme` package to do this, just as [(Piepho & Edmondson, 2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} did and you can find a direct comparison of the R syntax with additional information [in the MMFAIR chapter](https://schmidtpaul.github.io/MMFAIR/autoregressive_time_series.html#nlme-1){target="_blank"}. 

## MW - indepentent errors {.tabset .tabset-fade}

### glmmTMB

With the `glmmTMB()` function, it is possible to 

   1. fix the error variance to 0 via adding the `dispformula = ~ 0` argument and then 
   2. mimic the error (variance) as a random effect (variance) via the `unit` column with different entries for each data point.
   
While this may not be necessary for this model with the default homoscedastic, independent error variance structure, using it here will allow for an intuitve comparison to the following model with more sophisticated variance structures.

```{r}
mod.iid <- glmmTMB(formula = y ~ weekF * (variety + block) 
                        + (1 | unit),      # add random unit term to mimic error variance
                        dispformula = ~ 0, # fix original error variance to 0
                        REML = TRUE,       # needs to be stated since default = ML
                        data = dat) 

# Extract variance component estimates
# alternative: mod.iid %>% broom.mixed::tidy(effects = "ran_pars", scales = "vcov")
mod.iid %>% mixedup::extract_vc(ci_scale = "var") 
```

As expected, we find the residual variance to be 0 and instead we have the mimiced homoscedastic, independent error variance for the random `(1 | unit)` effect estimated as 0.023.

### nlme

Since the models in this chapter do not contain any random effects, we make use of `gls()` instead of `lme()`. Furthermore, we specifically write out the `correlation = NULL` argument to get a homoscedastic, independent error variance structure. While this may not be necessary because it is the default, using it here will allow for an intuitve comparison to the following models with more sophisticated variance structures.

```{r}
mod.iid.nlme <- gls(model = y ~ weekF * (block + variety), 
                    correlation = NULL, # default, i.e. homoscedastic, independent errors
                    data = dat)

# Extract variance component estimates
tibble(varstruct = "iid") %>% 
  mutate(sigma    = mod.iid.nlme$sigma) %>% 
  mutate(Variance = sigma^2)
```

It can be seen that the residual homoscedastic, independent error variance was estimated as 0.023.

## MW - autocorrelated errors {.tabset .tabset-fade .tabset-pills}

Note that at this point of the analysis, the model above with an independent, homogeneous error term above is neither the right, nor the wrong choice. It must be clear that *"measurements on the same plot are **likely** to be serially correlated"*. Thus, it should be investigated whether any covariance structure for the error term (instead of the default independence between errors) is more appropriate to model this dataset. It could theoretically be the case that this `mod.iid` is the best choice here, but we cannot confirm this yet, as we have not looked at any alternatives. This is what we will do in the next step.

> One may ask **at what step of the analysis it is best to compare and find the appropriate covariance structure for the error term** in such a scenario. It is indeed here, at this step. As [Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} write: *"**Before modelling the treatment effect**, a variance–covariance model needs to be identified for these correlations. This is best done by using a saturated model for treatments and time, i.e., a model that considers all treatment factors and time as qualitative."* Note that this *saturated model* is what we have in `mod.iid`. So in short: one should compare variance structures for the error term **before** running an ANOVA / conducting model selection steps. 

Units on which repeated observations are taken are often referred to as subjects. We would now like to allow measurements taken on the same subjects (plot in this case) to be serially correlated, while observations on different subjects are still considered independent. More specifically, we want the errors of the respective observations to be correlated in our model. 
<img src="img\corrvalues.PNG" style="width:60%; margin-right: 10px" align="left">

Take the visualisation on the left depicting a subset of our data. Here, you can see plots 1 and 2 (out of the total of 20 in our dataset) side by side. Furthermore, they are shown for weeks 1-3 (out of the total of 5 in our dataset). Thus, a total of six values are represented, coming from only two subjects/plots, but obtained in three different weeks. The blue arrows represent correlation among errors. For these six measurements, there are six blue arrows, since there are six error pairs that come from the same plot, respectively. The green lines on the other hand represent error pairs that do not come from the same plot and thus are assumed to be independent. Finally, notice that the errors coming from the same plot, but with two instead of just one week between their measurements, are shown in a lighter blue. This is because one may indeed assume a weaker correlation between errors that are further apart in terms of time passed between measurements.

The latter can be achieved by using a correlation structure for repeated measures over time. Maybe most popular correlation structure is called **first order autoregressive AR(1)**. It should be noted that this correlation structure is useful, if all time points are equally spaced, which is the case here, as there is always exactly one week between consecutive time points. As can be seen in the analysis of [*Example 4* in Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} and the corresponding [Agritutorial vigniette](https://cran.r-project.org/web/packages/agriTutorial/vignettes/agriTutorialVignette.pdf#%5B%7B%22num%22%3A81%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C28.346%2C813.543%2Cnull%5D){target="_blank"}, however, there are more options that just AR(1). Therefore, we will also try out multiple variance structures to model the serial correlation of our longitudinal data.

### AR(1) {.tabset .tabset-fade}

#### glmmTMB

To model the variance structure of our mimiced error term as first order autoregressive, we replace the `(1 | unit)` from the `iid` model with `ar1(weekF + 0 | plot)`. As can be seen, subjects are identified after the `|` and the variance structure in this syntax. 

Note that by adding the `show_cor = TRUE` argument to the `extract_vc` function, we obtain two outputs: The variance component estimates, as seen above for the `iid` model, as well as the correlation matrix / variance structure for a single plot.

```{r}
mod.AR1 <- glmmTMB(formula = y ~ weekF * (variety + block) +
                     ar1(weekF + 0 | plot), # ar1 structure as random term to mimic error var
                   dispformula = ~ 0, # fix original error variance to 0
                   REML = TRUE,       # needs to be stated since default = ML
                   data = dat) 

# Extract variance component estimates
# alternative: mod.ar1 %>% broom.mixed::tidy(effects = "ran_pars", scales = "vcov")
mod.AR1 %>% extract_vc(ci_scale = "var", show_cor = TRUE) 
```

We can see that the we get $\sigma^2_{plot} =$ `0.022` and a $\rho =$ `0.749`.

#### nlme

In `nlme` we make use of the `correlation =` argument and use the `corAR1` [correlation strucutre class](https://cran.r-project.org/web/packages/nlme/nlme.pdf#Rfn.corClasses.1){target="_blank"}. In its syntax, subjects are identified after the `|`.

```{r}
mod.AR1.nlme <- gls(model = y ~ weekF * (block + variety),
                    correlation = corAR1(form = ~ weekN | plot),
                    data = dat)

# Extract variance component estimates
tibble(varstruct = "ar(1)") %>%
  mutate(sigma    = mod.AR1.nlme$sigma,
         rho      = coef(mod.AR1.nlme$modelStruct$corStruct, unconstrained = FALSE)) %>%
  mutate(Variance = sigma^2,
         Corr1wk  = rho,
         Corr2wks = rho^2,
         Corr3wks = rho^3,
         Corr4wks = rho^4)
```

We can see that the we get $\sigma^2_{plot} =$ `0.022` and a $\rho =$ `0.749`.

#### nlme V2

For completeness, I want to show that the same model can also be fit in `nlme` via the `corExp` [correlation strucutre class](https://cran.r-project.org/web/packages/nlme/nlme.pdf#Rfn.corClasses.1){target="_blank"}. Note that this was actually done in the [Agritutorial vigniette](https://cran.r-project.org/web/packages/agriTutorial/vignettes/agriTutorialVignette.pdf#%5B%7B%22num%22%3A81%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C28.346%2C813.543%2Cnull%5D){target="_blank"} and [Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} write in Appendix 2: "Initially, we used corCAR1() to fit the model correlation structure but subsequently found that corExp() was more flexible as it allowed the inclusion of a “nugget” term, which seemingly cannot be done with corCAR1(). We note that it is essential that the variable weeks in the specification of the correlation structure, corr = corExp (form = ~weeks|Plots), is a numeric variable with values representing the time values of the repeated measurements. Note that this code will also work for repeated observations that are not necessarily equally spaced. It is also necessary that each plot has a unique level for the variable Plots."

Notice also that the increased flexibility comes at a price: Extracting the estimate for $\rho$ takes quite some code:

```{r}
mod.AR1.nlme.V2 <- gls(model = y ~ weekF * (variety + block),
                       correlation = corExp(form = ~ weekN | plot),
                       data = dat)

tibble(varstruct = "ar(1)") %>%
  mutate(sigma    = mod.AR1.nlme.V2$sigma,
         rho      = exp(-1/coef(mod.AR1.nlme.V2$modelStruct$corStruct, 
                                unconstrained = FALSE))) %>%
  mutate(Variance = sigma^2,
         Corr1wk  = rho,
         Corr2wks = rho^2,
         Corr3wks = rho^3,
         Corr4wks = rho^4)
```

We can see that the we get $\sigma^2_{plot} =$ `0.022` and a $\rho =$ `0.749`.

### AR(1) + nugget {.tabset .tabset-fade}

#### glmmTMB

</br></br></br>

<span style="color:red"> not possible ?</span>

</br></br></br>

```{r, eval=FALSE, class.source = "fold-hide"}
mod.AR1nugget <- glmmTMB(formula = y ~ weekF * (variety + block) +
                           ar1(weekF + 0 | plot), # ar1 structure as random term to mimic error var
                         # dispformula = ~ 0, # error variance allowed = nugget!
                         REML = TRUE,       # needs to be stated since default = ML
                         data = dat) 

# show variance components
# alternative: mod.AR1nugget %>% broom.mixed::tidy(effects = "ran_pars", scales = "vcov")
mod.AR1nugget %>% extract_vc(ci_scale = "var", show_cor = TRUE) 

# We can see that the we get $\sigma^2_{plot} =$ `0.019`, an additional residual/nugget variance  $\sigma^2_{N} =$ `0.004` and a $\rho =$ of `0.908`.
```

#### nlme

As pointed out by [Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} write in Appendix 2: "Initially, we used corCAR1() to fit the model correlation structure but subsequently found that corExp() was more flexible as it allowed the inclusion of a “nugget” term, which seemingly cannot be done with corCAR1(). We note that it is essential that the variable weeks in the specification of the correlation structure, corr = corExp (form = ~weeks|Plots), is a numeric variable with values representing the time values of the repeated measurements. Note that this code will also work for repeated observations that are not necessarily equally spaced. It is also necessary that each plot has a unique level for the variable Plots." This was also done in the [Agritutorial vigniette](https://cran.r-project.org/web/packages/agriTutorial/vignettes/agriTutorialVignette.pdf#%5B%7B%22num%22%3A81%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C28.346%2C813.543%2Cnull%5D){target="_blank"}.

Notice also that the increased flexibility comes at a price: Extracting the estimate for $\rho$ takes quite some code:

```{r}
mod.AR1nugget.nlme <- gls(model = y ~ weekF * (block + variety),
                          correlation = corExp(form = ~ weekN | plot, nugget = TRUE),
                          data = dat)

tibble(varstruct = "ar(1) + nugget") %>%
  mutate(sigma    = mod.AR1nugget.nlme$sigma,
         nugget   = coef(mod.AR1nugget.nlme$modelStruct$corStruct, 
                         unconstrained = FALSE)[2],
         rho      = (1-coef(mod.AR1nugget.nlme$modelStruct$corStruct, 
                            unconstrained = FALSE)[2])*
                    exp(-1/coef(mod.AR1nugget.nlme$modelStruct$corStruct, 
                                unconstrained = FALSE)[1])) %>%
  mutate(Variance = sigma^2,
         Corr1wk  = rho,
         Corr2wks = rho^2,
         Corr3wks = rho^3,
         Corr4wks = rho^4)
```

We can see that the we get $\sigma^2_{plot} =$ `0.0225` and a $\rho =$ `0.752`.

### CS {.tabset .tabset-fade}

#### glmmTMB (hCS!)

**Note that `glmmTMB()` only allows for heterogengeous compound symmetry and not standard compound symmetry!**

To model the variance structure of our mimiced error term as a heterogeneous compound symmetry structure, we use `cs(weekF + 0 | plot)`. Notice that this is not the simple compound symmetry structure that [Piepho & Edmondson (2018)](https://onlinelibrary.wiley.com/doi/full/10.1111/jac.12267){target="_blank"} applied, because `glmmTMB()` only allows for heterogeneous compound symmetry at the moment.

```{r}
mod.hCS <- glmmTMB(formula = y ~ weekF * (variety + block) +
                     cs(weekF + 0 | plot), # hcs structure as random term to mimic error var
                   dispformula = ~ 0, # fix original error variance to 0
                   REML = TRUE,       # needs to be stated since default = ML
                   data = dat) 

# show variance components
# alternative: mod.hCS %>% broom.mixed::tidy(effects = "ran_pars", scales = "vcov")
mod.hCS %>% extract_vc(ci_scale = "var", show_cor = TRUE) 
```

We can see that the we get heterogeneous $\sigma^2_{plot}$ estimates per week: `0.021`, `0.022`, `0.021`, `0.029`, `0.022` and a $\rho =$ `0.702`.

#### nlme

In `nlme` we make use of the `correlation =` argument and use the `corCompSymm` [correlation strucutre class](https://cran.r-project.org/web/packages/nlme/nlme.pdf#Rfn.corClasses.1){target="_blank"}. In its syntax, subjects are identified after the `|`.

```{r}
mod.CS.nlme <- gls(y ~ weekF * (block + variety),
                   corr = corCompSymm(form = ~ weekN | plot),
                   data = dat)

tibble(varstruct = "cs") %>%
  mutate(sigma    = mod.CS.nlme$sigma,
         rho      = coef(mod.CS.nlme$modelStruct$corStruct, unconstrained = FALSE)) %>%
  mutate(Variance = sigma^2,
         Corr1wk  = rho,
         Corr2wks = rho,
         Corr3wks = rho,
         Corr4wks = rho)
```

We can see that the we get $\sigma^2_{plot} =$ `0.023` and a $\rho =$ of `0.701`.

### Toeplitz {.tabset .tabset-fade}

#### glmmTMB

To model the variance structure of our mimiced error term as a toeplitz structure, we use `toep(weekF + 0 | plot)`. 

```{r}
mod.Toep <- glmmTMB(formula = y ~ weekF * (variety + block) +
                     toep(weekF + 0 | plot), # teop structure as random term to mimic err var
                   dispformula = ~ 0, # fix original error variance to 0
                   REML = TRUE,       # needs to be stated since default = ML
                   data = dat) 

# show variance components
# alternative: mod.Toep %>% broom.mixed::tidy(effects = "ran_pars", scales = "vcov")
mod.Toep %>% extract_vc(ci_scale = "var", show_cor = TRUE) 
```

We can see that the we get heterogeneous $\sigma^2_{plot}$ estimates per week: `0.021`, `0.023`, `0.022`, `0.029`, `0.020` and a heterogeneous $\rho$ per week-difference: `0.764`, `0.691`, `0.637` and `0.552`.

#### nlme

</br></br></br>

<span style="color:red"> not possible? At least it is not included in the agriTutorial vigniette and there is no obviously corresponding cor.class found in the nlme documentation. </span>

</br></br></br>

### UN {.tabset .tabset-fade}

#### glmmTMB

To model the variance structure of our mimiced error term as a completely unstrucutred variance structure, we use `us(weekF + 0 | plot)`.

```{r}
mod.UN <- glmmTMB(formula = y ~ weekF * (variety + block) +
                     us(weekF + 0 | plot), # us structure as random term to mimic error var
                   dispformula = ~ 0, # fix original error variance to 0
                   REML = TRUE,       # needs to be stated since default = ML
                   data = dat) 

# show variance components
# alternative: mod.UN %>% broom.mixed::tidy(effects = "ran_pars", scales = "vcov")
mod.UN %>% extract_vc(ci_scale = "var", show_cor = TRUE) 
```

As can be seen, each week *gets its own* variance and also each pair of weeks *gets* individual correlations.

#### nlme

In `nlme` we make use of the `correlation =` argument and use the `corSymm` [correlation strucutre class](https://cran.r-project.org/web/packages/nlme/nlme.pdf#Rfn.corClasses.1){target="_blank"} in combination with the [`varIdent()`](https://cran.r-project.org/web/packages/nlme/nlme.pdf#Rfn.varIdent.1){target="_blank"} in the [`weights=`](https://cran.r-project.org/web/packages/nlme/nlme.pdf#Rfn.lme.1){target="_blank"} argument, which is *used to allow for different variances according to the levels of a classification factor*. In its syntax, subjects are identified after the `|`.

```{r}
mod.UN.nlme <- gls(y ~ weekF * (block + variety), 
                   corr = corSymm(form = ~ 1 | plot), 
                   weights = varIdent(form = ~ 1|weekF), 
                   data = dat)

# Extract variance component estimates: variances
mod.UN.nlme$modelStruct$varStruct %>%
  coef(unconstrained = FALSE, allCoef = TRUE) %>% 
  enframe(name = "grp", value = "varStruct") %>%
  mutate(sigma         = mod.UN.nlme$sigma) %>%
  mutate(StandardError = sigma * varStruct) %>%
  mutate(Variance      = StandardError ^ 2)

# Extract variance component estimates: correlations
mod.UN.nlme$modelStruct$corStruct
```

As can be seen, each week *gets its own* variance and also each pair of weeks *gets* individual correlations.

# Model selection

## variance structure {.tabset .tabset-fade}

### glmmTMB

In order to select the best model here, we can simply compare their AIC values, since all models are identical regarding their fixed effects part. The smaller the value of AIC, the better is the fit:

```{r}
AICcmodavg::aictab(
  cand.set = list(mod.iid, mod.hCS, mod.AR1, mod.Toep, mod.UN), 
  modnames = c("iid", "hCS", "AR1", "Toeplitz", "UN"),
  second.ord = FALSE) # get AIC instead of AICc
```

According to the AIC value, the `mod.AR1` model is the best, suggesting that the plot values across weeks are indeed autocorrelated (as opposed to the `mod.iid` model) and that from all the potential variance structures, the first order autoregressive structure was best able to capture this autocorrelation.

### nlme

In order to select the best model here, we can simply compare their AIC values, since all models are identical regarding their fixed effects part. The smaller the value of AIC, the better is the fit:

```{r}
AICcmodavg::aictab(
  cand.set = list(mod.iid.nlme, mod.CS.nlme, mod.AR1.nlme, mod.AR1nugget.nlme, mod.UN.nlme), 
  modnames = c("iid", "CS", "AR1", "AR1 + nugget", "UN"),
  second.ord = FALSE) # get AIC instead of AICc
```

According to the AIC value, the `mod.AR1` model is the best, suggesting that the plot values across weeks are indeed autocorrelated (as opposed to the `mod.iid` model) and that from all the potential variance structures, the first order autoregressive structure was best able to capture this autocorrelation.

## time trend as (polynomial) regression model

So far, we have focussed on dealing with potentially correlated error terms for our repeated measures data. Now that we dealt with this and found an optimal solution, we are now ready to select a regression model for time trend. As a reminder, this is the data we are dealing with (this time not animated):

```{r}
ggplot(data = dat, 
       aes(y = y, x = weekF,
           group = variety,
           color = variety)) +
  geom_point(alpha = 0.75, size = 3) +
  stat_summary(fun=mean, geom="line") + # lines between means
  scale_y_continuous(
    name = "Leaf area index",
    limits = c(0, 6.5),
    expand = c(0, 0),
    breaks = c(0:6)) +
  scale_color_manual(values = var_colors) +
  theme_bw() +
  theme(legend.position = "bottom", 
        axis.title.x = element_blank())
```

It can now be asked whether the trend over time is simply linear (in the sense of `y = a + bx`) or can better be modelled as with a polynomial regression (*i.e.* `y = a + bx + cx²` or `y = a + bx + cx² + dx³` and so on). Very roughly put, we are asking whether a straight line fits the data well enough or if we should instead use a model that results in some sort of curved line. 

In order to answer this question, we use the *lack-of-fit* method to determine which degree of a polynomial regression we should go with. We start with the linear regression model.

</br></br></br>

<span style="color:red"> work in progress </span>

</br></br></br>

